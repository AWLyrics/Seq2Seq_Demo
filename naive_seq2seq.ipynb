{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from xpinyin import Pinyin\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 用小青龙的 time 一首歌做的 Seq2Seq 歌词生成 demo\n",
    "# 主要参照 https://github.com/ematvey/tensorflow-seq2seq-tutorials\n",
    "\n",
    "# pipeline: \n",
    "# 1. 上下两句作为 x 和 y\n",
    "# 2. jieba 切词\n",
    "# 3. word -> idx, padding, 记录词表\n",
    "# 4. 定义模型 seq2seq MLE\n",
    "# 5. 训练\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "x = []\n",
    "y =[]\n",
    "i = 0\n",
    "with open(\"time.txt\", \"r\") as f:\n",
    "    for l in f:\n",
    "        if i % 2 == 0:\n",
    "            x.append(l.strip())\n",
    "        else:\n",
    "            y.append(l.strip())\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拼音模块 押韵可能要用到\n",
    "# p = Pinyin()\n",
    "# for w in seg_list:\n",
    "#     if w != ' ':\n",
    "#         print(w, p.get_pinyin(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/0l/3x73_lfs6czgjngbxbtn1vrh0000gn/T/jieba.cache\n",
      "Loading model cost 0.914 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer \n",
    "tokenizer = Tokenizer(oov_token='<UNK>')\n",
    "words = []\n",
    "for s in x:\n",
    "    words.extend(jieba.cut(s))\n",
    "for s in y:\n",
    "    words.extend(jieba.cut(s))\n",
    "    \n",
    "tokenizer.fit_on_texts(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = []\n",
    "for s in x:\n",
    "    xx = tokenizer.texts_to_sequences(list(jieba.cut(s)))\n",
    "#     print(list(jieba.cut(s)))\n",
    "#     print(s)\n",
    "    if len(xx) > 0:\n",
    "        idx = [ xxx[0] for xxx in xx if xxx != []]\n",
    "        train.append(idx)\n",
    "test = []\n",
    "for s in y:\n",
    "    xx = tokenizer.texts_to_sequences(list(jieba.cut(s)))\n",
    "#     print(list(jieba.cut(s)))\n",
    "#     print(s)\n",
    "    if len(xx) > 0:\n",
    "        idx = [ xxx[0] for xxx in xx if xxx != []]\n",
    "        test.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_padded = tf.keras.preprocessing.sequence.pad_sequences(train, maxlen=12, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_padded = tf.keras.preprocessing.sequence.pad_sequences(test, maxlen=12, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 28, 120, 121,  20,   1, 122,   4, 123,   0,   0,   0,   0],\n",
       "       [124,  29,  18, 125, 126, 127, 128,   0,   0,   0,   0,   0],\n",
       "       [129, 130,   2, 131,   6,  10, 132,  13,   2,  21,   0,   0],\n",
       "       [133, 134, 135,  30,   5, 136,   0,   0,   0,   0,   0,   0],\n",
       "       [  1,  14,   1,  15,   7, 137, 138,   0,   0,   0,   0,   0],\n",
       "       [ 29, 139,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [140,   5, 141,   2, 142,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [ 31,   4, 143,  32,   1, 144, 145, 146, 147, 148,   0,   0],\n",
       "       [149,   1,  31,   4, 150, 151,   0,   0,   0,   0,   0,   0],\n",
       "       [152,  24, 153,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  1, 154,   9,  33,  34, 155,  32,   9,  33,  34, 156,   0],\n",
       "       [ 35, 157, 158, 159,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [ 19, 160, 161, 162,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [163, 164, 165,   2, 166, 167,   0,   0,   0,   0,   0,   0],\n",
       "       [  1, 168,  36, 169,  12,   3, 170,   0,   0,   0,   0,   0],\n",
       "       [171, 172, 173,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [174,   2, 175,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [ 28, 176,  27, 177,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [178, 179,   5,   1,   7, 180,   2, 181,   0,   0,   0,   0],\n",
       "       [182,  16, 183,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [184, 185,  36,  23,   2, 186,   0,   0,   0,   0,   0,   0],\n",
       "       [187, 188, 189,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  1, 190, 191, 192, 193,  35, 194,  30, 195,   0,   0,   0],\n",
       "       [196, 197,   2, 198, 199, 200,   1,   2, 201,   0,   0,   0],\n",
       "       [202,   5, 203, 204,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [ 17, 205,  26, 206, 207, 208,   0,   0,   0,   0,   0,   0]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_docs) + 2 # unk + oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define model \n",
    "hidden = 16\n",
    "embed_size = 32\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "    decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')\n",
    "    with tf.name_scope(\"embedding\"):\n",
    "        embeddings = tf.Variable(tf.random_uniform([vocab_size, embed_size], -1., 1.), dtype=tf.float32)\n",
    "        encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)\n",
    "        decoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, decoder_targets)\n",
    "    \n",
    "    with tf.name_scope(\"encoder\"):\n",
    "        encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden)\n",
    "        encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(encoder_cell, encoder_inputs_embedded, \n",
    "                                                                 dtype=tf.float32, time_major=True,)\n",
    "        del encoder_outputs\n",
    "    \n",
    "    with tf.name_scope(\"decoder\"):\n",
    "        decoder_cell = tf.contrib.rnn.LSTMCell(hidden)\n",
    "        # pass encoder final state as inital state\n",
    "        decoder_outputs, decoder_final_state = tf.nn.dynamic_rnn(\n",
    "                                                    decoder_cell, decoder_inputs_embedded,\n",
    "                                                        initial_state=encoder_final_state,\n",
    "                                                            dtype=tf.float32, time_major=True, scope=\"plain_decoder\")\n",
    "        decoder_logits = tf.contrib.layers.linear(decoder_outputs, vocab_size)\n",
    "        decoder_prediction = tf.argmax(decoder_logits, 2)\n",
    "#         print(decoder_logits)\n",
    "    \n",
    "    stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                                labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
    "                                    logits=decoder_logits,)\n",
    "\n",
    "    loss = tf.reduce_mean(stepwise_cross_entropy)\n",
    "    train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把输入变形为 time major\n",
    "def batch(inputs, max_sequence_length=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        inputs:\n",
    "            list of sentences (integer lists)\n",
    "        max_sequence_length:\n",
    "            integer specifying how large should `max_time` dimension be.\n",
    "            If None, maximum sequence length would be used\n",
    "    \n",
    "    Outputs:\n",
    "        inputs_time_major:\n",
    "            input sentences transformed into time-major matrix \n",
    "            (shape [max_time, batch_size]) padded with 0s\n",
    "        sequence_lengths:\n",
    "            batch-sized list of integers specifying amount of active \n",
    "            time steps in each input sequence\n",
    "    \"\"\"\n",
    "    \n",
    "    sequence_lengths = [len(seq) for seq in inputs]\n",
    "    batch_size = len(inputs)\n",
    "    \n",
    "    if max_sequence_length is None:\n",
    "        max_sequence_length = max(sequence_lengths)\n",
    "    \n",
    "    inputs_batch_major = np.zeros(shape=[batch_size, max_sequence_length], dtype=np.int32) # == PAD\n",
    "    \n",
    "    for i, seq in enumerate(inputs):\n",
    "        for j, element in enumerate(seq):\n",
    "            inputs_batch_major[i, j] = element\n",
    "\n",
    "    # [batch_size, max_time] -> [max_time, batch_size]\n",
    "    inputs_time_major = inputs_batch_major.swapaxes(0, 1)\n",
    "\n",
    "    return inputs_time_major, sequence_lengths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate indexes to sentence\n",
    "word2idx = tokenizer.word_index\n",
    "id2word = {k: v for v, k in zip(word2idx.keys(), word2idx.values())}\n",
    "def translate(word_indexs):\n",
    "    words = []\n",
    "    for idx in word_indexs:\n",
    "        word = id2word.get(idx)\n",
    "#         print(word)\n",
    "        if word:\n",
    "            words.append(id2word.get(idx))\n",
    "        else:\n",
    "            words.append(\"<UNK>\")\n",
    "    return \"\".join(words)\n",
    "#     print(words)\n",
    "#     print(\"\".join(words))\n",
    "#     print(\"Finish\")\n",
    "    \n",
    "print(translate([214, 1, 2,24 ,23, 3, 209, 214]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = \"当我没日没夜工作从长水起飞\"\n",
    "# s_idx = [ i[0] for i in tokenizer.texts_to_sequences(jieba.cut(s))]\n",
    "# print(s_idx)\n",
    "# ss = [s_idx]\n",
    "# test_input = tf.keras.preprocessing.sequence.pad_sequences(ss, maxlen=12, padding='post', truncating='post')\n",
    "# print(test_input)\n",
    "# print(test_input)\n",
    "# battle = batch(test_input, max_sequence_length=12)\n",
    "# print(battle[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_time, _ = batch(train_padded, max_sequence_length=12)\n",
    "test_time, _ = batch(test_padded, max_sequence_length=12)\n",
    "print(train_time.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(epochs):\n",
    "        l, _ = sess.run([loss, train_op], feed_dict={\n",
    "            encoder_inputs: train_time,\n",
    "            decoder_targets: test_time,\n",
    "        })\n",
    "        if e % 50 == 0:\n",
    "        \n",
    "            \n",
    "            pred = sess.run(decoder_prediction, feed_dict={\n",
    "                encoder_inputs: train_time,\n",
    "                decoder_targets: test_time\n",
    "            })\n",
    "            \n",
    "#             print(translate(np.transpose(pred)[0]))\n",
    "\n",
    "            rand_idx = (int) (random.random() * (train_time.shape[1]) )\n",
    "            print(\"epoch: %d loss : %f\" %( e, l))\n",
    "            print(\"input: \", translate(np.transpose(train_time)[rand_idx]))\n",
    "            print(\"predict: \", translate(np.transpose(pred)[rand_idx]))\n",
    "            print(\"ground truth: \",translate(np.transpose(test_time)[rand_idx]) )\n",
    "            print(\"-----------------------------------------\")\n",
    "#             print(\"Input: \")\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
